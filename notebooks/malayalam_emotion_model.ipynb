{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqbjHK-ohHlO"
      },
      "outputs": [],
      "source": [
        "# ðŸ“¦ Install dependencies\n",
        "!pip install -q imbalanced-learn lightgbm xgboost scikit-learn\n",
        "\n",
        "# ðŸ“š Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ðŸ“ Upload dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# ðŸ“„ Read dataset\n",
        "file_name = next(iter(uploaded))\n",
        "df = pd.read_csv(file_name) if file_name.endswith(\".csv\") else pd.read_excel(file_name)\n",
        "\n",
        "# ðŸŽ¯ Preprocessing\n",
        "df = df.drop(columns=[\"file_key\"], errors=\"ignore\")\n",
        "X = df.drop(columns=\"emotion\")\n",
        "y = df[\"emotion\"]\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Split text and other features\n",
        "text_data = X[\"text\"].fillna(\"\")\n",
        "X_non_text = X.drop(columns=\"text\").fillna(0)\n",
        "\n",
        "# Scale non-text features\n",
        "X_scaled = RobustScaler().fit_transform(X_non_text)\n",
        "\n",
        "# Feature selection\n",
        "X_selected = SelectKBest(f_classif, k=min(300, X_scaled.shape[1])).fit_transform(X_scaled, y_encoded)\n",
        "\n",
        "# Text vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=500)\n",
        "X_text_features = vectorizer.fit_transform(text_data)\n",
        "\n",
        "# Combine\n",
        "X_combined = hstack([csr_matrix(X_selected), X_text_features]).tocsr()\n",
        "\n",
        "# SMOTE\n",
        "X_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X_combined, y_encoded)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42)\n",
        "\n",
        "# Classifier\n",
        "lgbm = LGBMClassifier(n_estimators=200, max_depth=10, learning_rate=0.1, class_weight=\"balanced\", random_state=42)\n",
        "xgb = XGBClassifier(n_estimators=200, max_depth=10, learning_rate=0.1, eval_metric=\"mlogloss\", use_label_encoder=False, random_state=42)\n",
        "rf = RandomForestClassifier(n_estimators=200, max_depth=10, class_weight=\"balanced\", random_state=42)\n",
        "\n",
        "voting_clf = VotingClassifier(estimators=[(\"lgbm\", lgbm), (\"xgb\", xgb), (\"rf\", rf)], voting=\"soft\", n_jobs=-1)\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict Probabilities\n",
        "probs = voting_clf.predict_proba(X_test)\n",
        "class_labels = le.classes_\n",
        "n_classes = len(class_labels)\n",
        "\n",
        "# Find Best Thresholds\n",
        "best_thresholds = []\n",
        "for i in range(n_classes):\n",
        "    precision, recall, thresholds = precision_recall_curve((y_test == i).astype(int), probs[:, i])\n",
        "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
        "    best_thresh = thresholds[np.argmax(f1_scores)]\n",
        "    best_thresholds.append(best_thresh)\n",
        "    print(f\"ðŸ”§ Best threshold for class '{class_labels[i]}' = {best_thresh:.3f}\")\n",
        "\n",
        "# Correct Threshold Logic\n",
        "y_pred_custom = []\n",
        "for sample_probs in probs:\n",
        "    adjusted_probs = [prob if prob >= best_thresholds[i] else -np.inf for i, prob in enumerate(sample_probs)]\n",
        "    pred = np.argmax(adjusted_probs) if not np.all(np.isneginf(adjusted_probs)) else np.argmax(sample_probs)\n",
        "    y_pred_custom.append(pred)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nðŸ“Š Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred_custom, target_names=class_labels))\n",
        "print(f\"âœ… Accuracy: {accuracy_score(y_test, y_pred_custom):.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_custom)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix - Text + Non-Text Fusion\")\n",
        "plt.show()"
      ]
    }
  ]
}